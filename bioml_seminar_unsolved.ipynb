{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "bioml_seminar_unsolved.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteLabGx/home/blob/master/bioml_seminar_unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRGW1PXtulFJ"
      },
      "source": [
        "# Geometric Deep Learning for Molecules\n",
        "#### Sergei Grudinin, Ilia Igashov, Margot Selosse"
      ],
      "id": "xRGW1PXtulFJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHXdgAXIu4pG"
      },
      "source": [
        "This tutorial will start with the introduction to the PyTorch Geometric library. It will then present a basic description of graph-learning architectures, including convolution and attention operations. The first examples will include binary classification of 3D protein structures. After, we will apply the presented architectures to the regression task for the properties prediction of small molecules in the QM9 dataset. In the end, we will introduce more advanced architectures, specifically constructed to be rotation and translation equivariant, for the property predictions of 3D molecular graphs."
      ],
      "id": "LHXdgAXIu4pG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicked-boundary"
      },
      "source": [
        "# Contents\n",
        "\n",
        "- Prerequisites\n",
        "- PyTorch Geometric and NetworkX basics\n",
        "- Graph notations\n",
        "- Message Passing\n",
        "- Graph Convolutional Network (GCN)\n",
        "- Graph Attention Network (GAT)\n",
        "- Graph Classification on PROTEINS dataset\n",
        "- Graph Regression with QM9 dataset\n",
        "- SchNet and Equivariance\n",
        "- Further reading"
      ],
      "id": "wicked-boundary"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complex-assault"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "We will be using PyTorch Geometric and NetworkX – popular frameworks for working with graphs. \n",
        "\n"
      ],
      "id": "complex-assault"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swqT9LyNomJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497e3a85-1401-42c7-bf14-0de6174342c9"
      },
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
        "!pip install torch-geometric -f https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
        "!pip install ase\n",
        "\n",
        "!mkdir data"
      ],
      "id": "swqT9LyNomJw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcpu/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcpu/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (640 kB)\n",
            "\u001b[K     |████████████████████████████████| 640 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcpu/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 2.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcpu/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.9.0+cpu.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=dc79bd0ac4a756c3a9fe033c6226f2e32b6840702b026282018d5c3f09c144b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n",
            "Collecting ase\n",
            "  Downloading ase-3.22.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ase) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from ase) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from ase) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->ase) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.1.0->ase) (1.15.0)\n",
            "Installing collected packages: ase\n",
            "Successfully installed ase-3.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K16Tr5cK5N4D"
      },
      "source": [
        "In case you are going to run the notebook locally, you may need to do some additional installations (if you do not have this packages installed yet):\n",
        "\n",
        "* `numpy`\n",
        "* `matplotlib`\n",
        "* `torch`\n",
        "* `tqdm`\n",
        "* `networkx`"
      ],
      "id": "K16Tr5cK5N4D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "close-dairy"
      },
      "source": [
        "# PyTorch Geometric and NetworkX basics"
      ],
      "id": "close-dairy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stable-reconstruction"
      },
      "source": [
        "We will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) (PyG) framework as the main tool for working with graphs. It provides a convenient functionality for operating on:\n",
        "- [graph structures](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)\n",
        "- [graph-learning models](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html)\n",
        "- [common graph datasets](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html)\n",
        "\n",
        "Besides, [NetworkX](https://networkx.org/) (NX) library is useful for graph visualization."
      ],
      "id": "stable-reconstruction"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "municipal-legend"
      },
      "source": [
        "Let's create a simple graph with four nodes and five edges, where each node $v_i$ will have an associated feature value, its index $i$:\n",
        "\n",
        "<img align=\"middle\" src=\"https://www.researchgate.net/profile/Panayiota-Poirazi/publication/293945308/figure/fig1/AS:669375778529289@1536603030400/A-simple-graph-consisting-of-4-nodes-and-4-edges-The-degree-of-each-node-is.ppm\" width=\"200\"/>"
      ],
      "id": "municipal-legend"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "progressive-diamond"
      },
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import to_networkx"
      ],
      "id": "progressive-diamond",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respected-adaptation"
      },
      "source": [
        "# node features – indices:\n",
        "x = torch.tensor([[0], [1], [2], [3]], dtype=torch.long)\n",
        "\n",
        "# edges of the graph:\n",
        "edge_index = torch.tensor(\n",
        "    [\n",
        "        [0, 0, 0, 1, 1, 2, 3, 3],\n",
        "        [1, 2, 3, 0, 3, 0, 0, 1],\n",
        "    ], \n",
        "    dtype=torch.long\n",
        ")\n",
        "\n",
        "# PyG graph:\n",
        "pg_graph = Data(x=x, edge_index=edge_index)"
      ],
      "id": "respected-adaptation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apparent-medicaid"
      },
      "source": [
        "Let's look at the created PyG graph object:"
      ],
      "id": "apparent-medicaid"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dense-thinking"
      },
      "source": [
        "pg_graph"
      ],
      "id": "dense-thinking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "native-psychiatry"
      },
      "source": [
        "Useful graph attributes:"
      ],
      "id": "native-psychiatry"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recent-shame"
      },
      "source": [
        "print('Number of nodes in the graph:', pg_graph.num_nodes)\n",
        "print('Number of edges in the graph:', pg_graph.num_edges)\n",
        "print('Number of node features:', pg_graph.num_node_features)"
      ],
      "id": "recent-shame",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "public-optimum"
      },
      "source": [
        "Note that `edge_index`, i.e. the tensor defining the source and target nodes of all edges, **is not a list of index tuples**. If you want to write your indices this way, you should transpose and call `contiguous` on it before passing them to the data constructor:"
      ],
      "id": "public-optimum"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expanded-spray"
      },
      "source": [
        "edge_index = torch.tensor(\n",
        "    [\n",
        "        [0, 1],\n",
        "        [0, 2],\n",
        "        [0, 3],\n",
        "        [1, 0],\n",
        "        [1, 3], \n",
        "        [2, 0],\n",
        "        [3, 0],\n",
        "        [3, 1],\n",
        "    ], \n",
        "    dtype=torch.long\n",
        ")\n",
        "\n",
        "edge_index.t().contiguous()"
      ],
      "id": "expanded-spray",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sized-portugal"
      },
      "source": [
        "Let's transform PyG graph to the NX format and draw the resulting graph:"
      ],
      "id": "sized-portugal"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blank-walker"
      },
      "source": [
        "# transform:\n",
        "nx_graph = to_networkx(pg_graph)\n",
        "\n",
        "# draw:\n",
        "nx.draw(\n",
        "    nx_graph, \n",
        "    font_size=10,\n",
        "    width=0.5, \n",
        "    with_labels=True,\n",
        "    labels={i: f'v{pg_graph.x[i][0]}' for i in range(pg_graph.num_nodes)},\n",
        ")"
      ],
      "id": "blank-walker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hungry-draft"
      },
      "source": [
        "Now let's create the second simple graph with two nodes and one edge:"
      ],
      "id": "hungry-draft"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongoing-convergence"
      },
      "source": [
        "second_pg_graph = Data(\n",
        "    x=torch.tensor([[10], [11]], dtype=torch.long),\n",
        "    edge_index=torch.tensor([[0, 1], [1, 0]], dtype=torch.long),\n",
        ")\n",
        "\n",
        "second_nx_graph = to_networkx(second_pg_graph)\n",
        "\n",
        "nx.draw(\n",
        "    second_nx_graph, \n",
        "    font_size=10,\n",
        "    width=0.5, \n",
        "    with_labels=True,\n",
        "    labels={i: f'v{second_pg_graph.x[i][0]}' for i in range(second_pg_graph.num_nodes)},\n",
        ")"
      ],
      "id": "ongoing-convergence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nasty-hospital"
      },
      "source": [
        "Let's put both graphs into DataLoader with `batch_size=2`:"
      ],
      "id": "nasty-hospital"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thirty-shelf"
      },
      "source": [
        "loader = DataLoader([pg_graph, second_pg_graph], batch_size=2)"
      ],
      "id": "thirty-shelf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proprietary-motion"
      },
      "source": [
        "Let's check how many batches the loader is going to generate:"
      ],
      "id": "proprietary-motion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appreciated-drunk"
      },
      "source": [
        "len(loader)"
      ],
      "id": "appreciated-drunk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "active-estimate"
      },
      "source": [
        "Let's see what the loader yields:"
      ],
      "id": "active-estimate"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scientific-verse"
      },
      "source": [
        "for batch in loader:\n",
        "    print(batch)"
      ],
      "id": "scientific-verse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expected-season"
      },
      "source": [
        "The batch object is very similar to the `Data` object, but it has additonal attribute `ptr` that defines ranges of nodes' indices that belong to different graphs in the batch:"
      ],
      "id": "expected-season"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effective-broadcast"
      },
      "source": [
        "batch.ptr"
      ],
      "id": "effective-broadcast",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "provincial-directive"
      },
      "source": [
        "That means that nodes with indices $0\\leq{i}<4$ belong to the first graph in the batch, and nodes with indices $4\\leq{i}<6$ belong to the second graph in a batch. For checking that, let's take a look at the attribute `x` of the batch:"
      ],
      "id": "provincial-directive"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "radical-blind"
      },
      "source": [
        "batch.x"
      ],
      "id": "radical-blind",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complimentary-disabled"
      },
      "source": [
        "So the batch of size $B$ can be thought as a new big graph that contains $B$ disjoint components corresponding to the graphs included in this batch:"
      ],
      "id": "complimentary-disabled"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "square-brunei"
      },
      "source": [
        "nx_batch = to_networkx(batch)\n",
        "\n",
        "nx.draw(\n",
        "    nx_batch, \n",
        "    font_size=10,\n",
        "    width=0.5, \n",
        "    with_labels=True,\n",
        "    labels={i: f'v{batch.x[i][0]}' for i in range(batch.num_nodes)},\n",
        ")"
      ],
      "id": "square-brunei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWb3UrFwdpoy"
      },
      "source": [
        "# Graph notations\n",
        "\n",
        "Let's consider an *undirected graph* $G=(V, E)$, where $V$ is the set of nodes and $E$ is the set of edges.\n",
        "For a graph node $u\\in V$, we define its neighborhood as \n",
        "\n",
        "$$\n",
        "N(u)=\\{v\\in{V}\\ |\\ (u,v)\\in E\\}.\n",
        "$$\n",
        "\n",
        "The adjacency matrix $\\boldsymbol{A}$ of graph $G$ is a square $|V|\\times|V|$ symmetric matrix where each entry relates to an edge between the corresponding nodes. In case of the *weigted* graph, when each edge $(v_i,v_j)\\in E$ has weight $w_{ij}\\in\\mathbb{R}$, the corresponding entry of the adjacency matrix $a_{ij}$ equals to this weight:\n",
        "\n",
        "$$\n",
        "\\forall {i,j}\\in\\{1,\\dots,|V|\\}\\ \\ \\ \\ a_{ij}=\n",
        "\\begin{cases}\n",
        "w_{ij},\\ &\\text{if}\\ (v_i,v_j)\\in{E},\\\\\n",
        "0,\\ &\\text{if}\\ (v_i,v_j)\\notin{E}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "In case of the *unweighted* graph, the adjacency matrix is binary:\n",
        "\n",
        "$$\n",
        "\\forall {i,j}\\in\\{1,\\dots,|V|\\}\\ \\ \\ \\ a_{ij}=\n",
        "\\begin{cases}\n",
        "1,\\ &\\text{if}\\ (v_i,v_j)\\in{E},\\\\\n",
        "0,\\ &\\text{if}\\ (v_i,v_j)\\notin{E}.\n",
        "\\end{cases}\n",
        "$$"
      ],
      "id": "vWb3UrFwdpoy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "documentary-album"
      },
      "source": [
        "# Message Passing\n",
        "\n",
        "Graph Neural Networks (GNNs) rely on a more generic framework referred to as \"Message Passing\" that proceeds as follows.\n",
        "\n",
        "Assume that each node $v_i$ of the input graph $G$ has an associated vector of features $\\boldsymbol{z}_i^{0}$ of size $d^0$. Consider $K$ message-passing layers. \n",
        "\n",
        "For $k \\in \\{1,\\ldots,K\\}$:\n",
        "\n",
        "* For all nodes $v_i$ and for all its neighbours, we build a message $\\color{red}{\\boldsymbol{m}_{ij}^{k}}$ with some differentiable function $\\phi$:\n",
        "\n",
        "$$\n",
        "\\color{red}{\\boldsymbol{m}_{ij}^{k} \\leftarrow \\phi(z_i^{k-1},z_j^{k-1})},\\tag{1}\n",
        "$$\n",
        "\n",
        "\n",
        "* We aggregate the messages in $\\color{green}{\\boldsymbol{h}_i^{k}}$, a vector of size $d^{k-1}$, using some differentiable and permutation invariant function AGGR:\n",
        "\n",
        "$$\n",
        "\\color{green}{\\boldsymbol{h}_i^{k} \\leftarrow \\text{AGGR}(\\color{red}{\\boldsymbol{m}_{ij}^{k}}, \\forall v_j \\in N(v_i) \\cup \\{v_i\\})},\\tag{2}\n",
        "$$\n",
        "\n",
        "* We build a new embedding $\\boldsymbol{z}^{k}_i$ for each node $v_i$ with $\\boldsymbol{W}^{k}$ a matrix of size $d^{k}\\times d^{k-1}$:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{z}^{k}_i \\leftarrow \\sigma(\\boldsymbol{W}^{k} . \\color{green}{\\boldsymbol{h}_i^{k}}).\\tag{3}\n",
        "$$\n",
        "    \n",
        "\n",
        "Finally, we set the embedding of node $i$ as $\\boldsymbol{z}_i=\\boldsymbol{z}^{K}_i$.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*fPzRm3Flq3dQErn7LEG_Ig.png\" width=\"900\"/>\n",
        "</div>"
      ],
      "id": "documentary-album"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alive-simpson"
      },
      "source": [
        "Note that the user has to choose:\n",
        "\n",
        "* the number of layers $K$,\n",
        "* The $\\phi$ function,\n",
        "* the AGGR function, which is an aggregation function (e.g: max, sum, mean),\n",
        "* the dimensions $d^{k}$ for $k \\geq 1$\n",
        "* $\\sigma$, which is a non-linear function (e.g ReLU)."
      ],
      "id": "alive-simpson"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "streaming-broadcasting"
      },
      "source": [
        "## Graph Convolutional Network\n",
        "\n",
        "* [original paper](https://arxiv.org/abs/1609.02907)\n",
        "* [original code](https://github.com/tkipf/gcn)\n",
        "\n",
        "The Graph Convolutional Network (GCN) is a graph neural network that implements the Message Passing framework such that:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{z}'_i = \\sigma\\left[\n",
        "\\color{green}{\\sum_{v_j\\in{N}(v_i)\\cup\\{v_i\\}}}\n",
        "\\color{red}{\n",
        "\\frac{1}{\\sqrt{\\text{deg}(v_i)}\\sqrt{\\text{deg}(v_j)}} \n",
        "}\n",
        "\\boldsymbol{\\Theta}\\boldsymbol{z}_j\\right]\n",
        ",\\tag{4}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{\\Theta}$ is a weight matrix, and $\\text{deg}(v)$ is a degree of the node $v$:\n",
        "\n",
        "$$\n",
        "\\text{deg}(v) = \\sum_{u\\in{V}}\\mathbb{I}\\{(u,v)\\in{E}\\}.\\tag{5}\n",
        "$$\n",
        "\n",
        "**Note:** for simplicity, we dropped indices $k$ corresponding to the layer's number. Instead, we use prime `'` as an indication of the updated embeddings. We assume that each node $v_i$ has an embedding vector $\\boldsymbol{z}_i\\in\\mathbb{R}^d$ before applying the convolution layer, and gets an updated embedding vector $\\boldsymbol{z}'_i\\in\\mathbb{R}^{d'}$ after applying the convolution layer."
      ],
      "id": "streaming-broadcasting"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synthetic-shade"
      },
      "source": [
        "## Graph Attention Network\n",
        "\n",
        "* [original paper](https://arxiv.org/abs/1710.10903)\n",
        "* [original code](https://github.com/PetarV-/GAT)\n",
        "\n",
        "\n",
        "The Graph Attention Network (GAT) is a graph neural network that implements the Message Passing framework such that:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{z}'_i = \\sigma\\left[\n",
        "\\color{green}{\\sum_{v_j\\in{N}(v_i)\\cup\\{v_i\\}}}\n",
        "\\color{red}{\n",
        "\\alpha_{ij} \n",
        "}\n",
        "\\boldsymbol{\\Theta}\\boldsymbol{z}_j\\right]\n",
        ",\\tag{6}\n",
        "$$\n",
        "\n",
        "where attention coefficients $\\alpha_{ij}$ are computed as follows,\n",
        "\n",
        "$$\n",
        "\\alpha_{ij}=\\frac{\n",
        "\\exp\\big(\\text{LeakyReLU}\\big(\\boldsymbol{a}^{\\text{T}}[\\boldsymbol{\\Theta}\\boldsymbol{z}_i||\\boldsymbol{\\Theta}\\boldsymbol{z}_j]\\big)\\big)\n",
        "}{\n",
        "\\sum_{v_m\\in{N(v_i)}}\\exp\\big(\\text{LeakyReLU}\\big(\\boldsymbol{a}^{\\text{T}}[\\boldsymbol{\\Theta}\\boldsymbol{z}_i||\\boldsymbol{\\Theta}\\boldsymbol{z}_m]\\big)\\big)\n",
        "}.\\tag{7}\n",
        "$$\n",
        "\n",
        "Here, $||$ represents concatenation, and $\\boldsymbol{a}\\in\\mathbb{R}^{2d'}$ is a vector of learnable parameters.\n",
        "\n",
        "**Note:** for simplicity, we dropped indices $k$ corresponding to the layer's number. Instead, we use prime `'` as an indication of the updated embeddings. We assume that each node $v_i$ has an embedding vector $\\boldsymbol{z}_i\\in\\mathbb{R}^d$ before applying the convolution layer, and gets an updated embedding vector $\\boldsymbol{z}'_i\\in\\mathbb{R}^{d'}$ after applying the convolution layer."
      ],
      "id": "synthetic-shade"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "looMNPALwwBs"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/1036/1*3D844_twutCaunYMPuo-Sw.png\" width=\"400\"/>\n",
        "</div>"
      ],
      "id": "looMNPALwwBs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "organic-antique"
      },
      "source": [
        "To stabilize the learning process of self-attention, we use *multi-head attention*. To do this we use $L$ independent attention mechanisms, or “heads” compute output features. Then, we aggregate these output feature representations:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{z}'_i = \\sigma\\left[\n",
        "\\sum_{l=1}^{L}\\sum_{v_j\\in{N}(v_i)\\cup\\{v_i\\}}\n",
        "\\alpha_{ij}^{(l)} \n",
        "\\boldsymbol{\\Theta}^{(l)}\\boldsymbol{z}_j\\right].\\tag{8}\n",
        "$$"
      ],
      "id": "organic-antique"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "equivalent-pickup"
      },
      "source": [
        "# Graph classification: PROTEINS dataset"
      ],
      "id": "equivalent-pickup"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "violent-architecture"
      },
      "source": [
        "PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6Å apart.\n",
        "\n",
        "This dataset can be obtained from PyTorch Geometric: https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html"
      ],
      "id": "violent-architecture"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "considerable-chester"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset"
      ],
      "id": "considerable-chester",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electric-metropolitan"
      },
      "source": [
        "dataset = TUDataset(root='data/PROTEINS', name='PROTEINS').shuffle()"
      ],
      "id": "electric-metropolitan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opposite-scenario"
      },
      "source": [
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Number of node features: {dataset.num_features}')"
      ],
      "id": "opposite-scenario",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMR45P2cpIZY"
      },
      "source": [
        "Let's vizualise several graphs using NetworkX:"
      ],
      "id": "JMR45P2cpIZY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "personal-image"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "COLORS = [\n",
        "    '#C3EFFC',\n",
        "    '#FCC4C3',\n",
        "    '#FCF9C3',\n",
        "    '#E1FCC3',\n",
        "    '#C1EFBF',\n",
        "    '#BFC9EF',\n",
        "    '#CCBFEF',\n",
        "    '#EBBFEF',\n",
        "    '#CCB4C4',\n",
        "    '#EEEEEE',\n",
        "]\n",
        "\n",
        "def draw_colored_graph(nx_graph, colors, labels, ax=None):\n",
        "    if ax is None:\n",
        "        plt.figure(figsize=(20, 12))\n",
        "    nx.draw(\n",
        "        nx_graph, \n",
        "        node_color=colors,  \n",
        "        font_size=10, \n",
        "        width=0.2, \n",
        "        with_labels=True,\n",
        "        labels=labels,\n",
        "        ax=ax\n",
        "    )"
      ],
      "id": "personal-image",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blocked-lying"
      },
      "source": [
        "n_examples = 5\n",
        "\n",
        "fix, ax = plt.subplots(nrows=1, ncols=n_examples, figsize=(5*n_examples, 5))\n",
        "\n",
        "for i, rand_ix in enumerate(np.random.choice(dataset.indices(), 5)):\n",
        "    curr_ax = ax[i]\n",
        "    curr_ax.set_title(f'Graph with id={rand_ix}')\n",
        "    \n",
        "    pg_graph = dataset[rand_ix]\n",
        "    nx_graph = to_networkx(pg_graph, to_undirected=True)\n",
        "    colors = [COLORS[np.argmax(features)] for features in pg_graph.x]\n",
        "    \n",
        "    draw_colored_graph(nx_graph, colors, labels={}, ax=curr_ax)"
      ],
      "id": "blocked-lying",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "norman-mortality"
      },
      "source": [
        "## 1. Graph classification with GCN"
      ],
      "id": "norman-mortality"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unlikely-breathing"
      },
      "source": [
        "At first, let's create data loaders for training, validation and testing. For that, we will use [PyG DataLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader) that combines input graphs into batches. One batch is represented as a single graph with multiple disconnected components."
      ],
      "id": "unlikely-breathing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excess-television"
      },
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "batch_size = 4\n",
        "loader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "id": "excess-television",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inside-productivity"
      },
      "source": [
        "# Let's pick the first batch and visualize it.\n",
        "# We will see that it is a simple graph with multiple disconnected components.\n",
        "# Each component corresponds to a graph in the initial dataset.\n",
        "# Here for illustration we label and color nodes according to these components\n",
        "\n",
        "batch = loader.__iter__().next()\n",
        "nx_graph = to_networkx(batch, to_undirected=True)\n",
        "\n",
        "mask = np.concatenate([\n",
        "    np.ones(batch.ptr[i+1] - batch.ptr[i], dtype=int) * i\n",
        "    for i in range(batch_size) \n",
        "])\n",
        "colors = [COLORS[graph_idx] for graph_idx in mask]\n",
        "labels = dict(zip(range(len(mask)), mask))\n",
        "\n",
        "\n",
        "draw_colored_graph(nx_graph, colors=colors, labels=labels)"
      ],
      "id": "inside-productivity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surgical-chosen"
      },
      "source": [
        "batch_size = 32\n",
        "data_size = len(dataset)\n",
        "\n",
        "train_loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset[int(data_size * 0.8):int(data_size * 0.9)], batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=batch_size)"
      ],
      "id": "surgical-chosen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taken-sleep"
      },
      "source": [
        "### Task 1.1 – Implement GCN Layer"
      ],
      "id": "taken-sleep"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plastic-donor"
      },
      "source": [
        "Pytorch Geometric provides the [Message Passing interface](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) that contains three main functions:\n",
        "* `message()`, which defines how the message $\\color{green}{\\boldsymbol{m}_{ij}^{k}}$ is built,\n",
        "* `aggregate()`, which defines how the messages are aggregated into $\\color{red}{\\boldsymbol{h}_{i}^{k}},$ \n",
        "* `propagate()`, which calls the `message()`, `aggregate()` functions.  \n",
        "\n",
        "We will implement GCN layer according to the formula (4) on top this interface."
      ],
      "id": "plastic-donor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "catholic-evening"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree"
      ],
      "id": "catholic-evening",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "constitutional-subsection"
      },
      "source": [
        "class GCN(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__(aggr='add')\n",
        "         \n",
        "        # Create a learnable linear parameter Theta used in formula (4)\n",
        "        # YOUR CODE HERE\n",
        "        self.lin = ...\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Linearly transform node feature matrix.\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "\n",
        "        # Compute normalization.\n",
        "        # Hint: function `degree` from torch_geometric.utils can be useful here\n",
        "        # YOUR CODE HERE (~5 lines):\n",
        "        norm = ...\n",
        "\n",
        "        # Start propagating messages.\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # x_j has shape [E, out_channels]\n",
        "\n",
        "        # Normalize node features.\n",
        "        return norm.view(-1, 1) * x_j"
      ],
      "id": "constitutional-subsection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regulated-leader"
      },
      "source": [
        "# Check:\n",
        "out_channels = 10\n",
        "gcn_layer = GCN(in_channels=dataset.num_node_features, out_channels=out_channels)\n",
        "\n",
        "rand_idx = np.random.randint(0, len(dataset))\n",
        "graph = dataset[rand_idx]\n",
        "output = gcn_layer(graph.x, graph.edge_index)\n",
        "\n",
        "if output.shape == torch.Size([graph.num_nodes, out_channels]): \n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Error: layer should output a 2-dimensional tensor of shape (N, out_channels)')"
      ],
      "id": "regulated-leader",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "straight-phrase"
      },
      "source": [
        "### Task 1.2 – Implement GNN with GCN"
      ],
      "id": "straight-phrase"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "certain-convergence"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.0):\n",
        "        super(GNN, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create a sequence of GCN layers with non-linearities (ReLU)\n",
        "        # Hint: consider pyg_nn.Sequential, an extension of the torch.nn.Sequential\n",
        "        # YOUR CODE HERE:\n",
        "        self.convs = ...\n",
        "\n",
        "        # Create post-message-passing linear transformations and aggregations\n",
        "        # Hint: try a couple of linear layers with non-linearities (ReLU) and dropout\n",
        "        # YOUR CODE HERE:\n",
        "        self.post_mp = ...\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        \n",
        "        # Apply GCN layers with non-linearities (ReLU)\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "        \n",
        "        # Aggregate node embeddings tensor of shape (N, hidden_dim) \n",
        "        # to get the graph embedding tensor of shape (hidden_dim).\n",
        "        # Hint 1: pyg_nn.global_max_pool can be useful\n",
        "        # Hint 2: keep in mind that the data is batched\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "        \n",
        "        # Apply post-message-passing transformations\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "\n",
        "        return x"
      ],
      "id": "certain-convergence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dirty-invasion"
      },
      "source": [
        "# Check:\n",
        "hidden_dim = 10\n",
        "gnn = GNN(\n",
        "    input_dim=dataset.num_node_features, \n",
        "    hidden_dim=hidden_dim, \n",
        "    output_dim=dataset.num_classes, \n",
        ")\n",
        "\n",
        "batch = loader.__iter__().next()\n",
        "output = gnn(batch)\n",
        "\n",
        "if output.shape == torch.Size([loader.batch_size, dataset.num_classes]): \n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Error: GNN should output a 2-dimensional tensor of shape (batch_size, num_classes)')"
      ],
      "id": "dirty-invasion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "persistent-implement"
      },
      "source": [
        "### Task 1.3 – Train GNN with GCN to predict classes of protein graphs"
      ],
      "id": "persistent-implement"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knowing-count"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def cross_entropy_loss(x, labels):\n",
        "    return F.cross_entropy(x, labels)\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, val_loader, epochs):\n",
        "    train_loss = []\n",
        "    val_accuracy = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        batch_train_loss = []\n",
        "        batch_val_accuracy = []\n",
        "\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            # Get logits from the model\n",
        "            # YOUR CODE HERE:\n",
        "            logits = ...\n",
        "            \n",
        "            # Get ground-truth labels\n",
        "            # YOUR CODE HERE:\n",
        "            labels = ...\n",
        "\n",
        "            # Calculate loss\n",
        "            # YOUR CODE HERE:\n",
        "            loss = ...\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_train_loss.append(float(loss.data.numpy()))\n",
        "\n",
        "        train_loss.append(np.mean(batch_train_loss))\n",
        "\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            # Get predictions from the model\n",
        "            # YOUR CODE HERE:\n",
        "            pred = ...\n",
        "\n",
        "            # Get ground-truth labels\n",
        "            # YOUR CODE HERE:\n",
        "            labels = ...\n",
        "            \n",
        "            batch_val_accuracy.append(np.mean((labels == pred).numpy()))\n",
        "\n",
        "        val_accuracy.append(np.mean(batch_val_accuracy))\n",
        "        \n",
        "    return model, train_loss, val_accuracy\n",
        "\n",
        "\n",
        "def plot_progress(train_loss, val_accuracy):\n",
        "    fig, (train_ax, val_ax) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "\n",
        "    train_ax.plot(train_loss)\n",
        "    train_ax.set_title('Train loss')\n",
        "    train_ax.set_xlabel('Epoch')\n",
        "\n",
        "    val_ax.plot(val_accuracy)\n",
        "    val_ax.set_title('Val accuracy')\n",
        "    val_ax.set_xlabel('Epoch')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = np.array([])\n",
        "    labels = np.array([])\n",
        "\n",
        "    for batch in loader:\n",
        "\n",
        "        # Get predicted labels\n",
        "        # YOUR CODE HERE:\n",
        "        pred = ...\n",
        "\n",
        "        # Get ground-truth labels\n",
        "        # YOUR CODE HERE:\n",
        "        true = ...\n",
        "\n",
        "        predictions = np.append(predictions, pred)\n",
        "        labels = np.append(labels, true)\n",
        "\n",
        "    return np.mean(predictions == labels)"
      ],
      "id": "knowing-count",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "distinct-photographer"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create GNN\n",
        "# YOUR CODE HERE:\n",
        "model = ...\n",
        "optimizer = ..."
      ],
      "id": "distinct-photographer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interpreted-holmes"
      },
      "source": [
        "Training:"
      ],
      "id": "interpreted-holmes"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "square-customs"
      },
      "source": [
        "epochs = 50\n",
        "model, train_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, epochs)\n",
        "plot_progress(train_loss, val_accuracy)"
      ],
      "id": "square-customs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "color-nickname"
      },
      "source": [
        "Evaluation:"
      ],
      "id": "color-nickname"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dress-mayor"
      },
      "source": [
        "accuracy = evaluate(model, test_loader)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "if accuracy >= 0.7:\n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Try better!')"
      ],
      "id": "dress-mayor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aware-louisiana"
      },
      "source": [
        "## 2. Graph classification with GAT"
      ],
      "id": "aware-louisiana"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imperial-realtor"
      },
      "source": [
        "Let's now implement Graph Attention Layer and perform the same graph classification procedure on PROTEINS dataset with a new GNN that contains GAT layers."
      ],
      "id": "imperial-realtor"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "applicable-priest"
      },
      "source": [
        "### Task 2.1 – Implement GAT Layer"
      ],
      "id": "applicable-priest"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heated-oriental"
      },
      "source": [
        "We will implement multi-head GAT layer according to the formula (8) on top PyG Message-Passing interface."
      ],
      "id": "heated-oriental"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iraqi-duplicate"
      },
      "source": [
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "class GAT(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, num_heads):\n",
        "        super(GAT, self).__init__(aggr='add')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # Create a learnable linear parameter Theta used in formula (6)\n",
        "        # Hint: keep in mind that we have multiple independent heads\n",
        "        # YOUR CODE HERE:\n",
        "        self.lin = ...\n",
        "        \n",
        "        # Create a learnable attention vector that is used in formula (7)\n",
        "        # YOUR CODE HERE:\n",
        "        self.att = ...\n",
        "        \n",
        "        # Initialization of the attention vector\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        \n",
        "        # Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        # Apply linear transformation to the node feature matrix.\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "\n",
        "        # Start propagating messages.\n",
        "        return self.propagate(edge_index=edge_index, x=x)\n",
        "\n",
        "    def message(self, x_i, x_j, index):\n",
        "        # Constructs messages to node i for each edge (j, i).\n",
        "        # x_i – feature vectors of target nodes corresponding to i-th index, i.e. x[edge_index[1]]\n",
        "        # x_j – feature vectors of source nodes corresponding to j-th index, i.e. x[edge_index[0]]\n",
        "        # index – target (i-th) nodes indices, i.e. edge_index[1]\n",
        "\n",
        "        # Compute the attention coefficients alpha as described in equation (7).\n",
        "        # Remember to be careful of the number of heads with dimension!\n",
        "        # Hint: function pyg_utils.softmax can be useful here\n",
        "        # YOUR CODE HERE (~6 lines):\n",
        "        alpha = ...\n",
        "        \n",
        "        return (alpha * x_j).mean(dim=1)"
      ],
      "id": "iraqi-duplicate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bottom-survey"
      },
      "source": [
        "# Check:\n",
        "out_channels = 10\n",
        "num_heads = 3\n",
        "gat_layer = GAT(in_channels=dataset.num_node_features, out_channels=out_channels, num_heads=num_heads)\n",
        "\n",
        "rand_idx = np.random.randint(0, len(dataset))\n",
        "graph = dataset[rand_idx]\n",
        "output = gat_layer(graph.x, graph.edge_index)\n",
        "\n",
        "if output.shape == torch.Size([graph.num_nodes, out_channels]): \n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Error: layer should output a 2-dimensional tensor of shape (N, out_channels)')"
      ],
      "id": "bottom-survey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "precious-surgeon"
      },
      "source": [
        "### Task 2.2 – Implement GNN with GAT"
      ],
      "id": "precious-surgeon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polished-clearing"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, dropout=0.0):\n",
        "        super(GNN, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create a sequence of GAT layers\n",
        "        # YOUR CODE HERE:\n",
        "        self.convs = ...\n",
        "\n",
        "        # Create post-message-passing linear transformations and aggregations\n",
        "        # Hint: try a couple of linear layers with non-linearities (ReLU) and dropout\n",
        "        # YOUR CODE HERE:\n",
        "        self.post_mp = ...\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        \n",
        "        # Apply GAT layers with non-linearities (ReLU)\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "        \n",
        "        # Aggregate node embeddings tensor of shape (N, hidden_dim) \n",
        "        # to get the graph embedding tensor of shape (hidden_dim).\n",
        "        # Hint 1: pyg_nn.global_max_pool can be useful\n",
        "        # Hint 2: keep in mind that the data is batched\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "        \n",
        "        # Apply post-message-passing transformations\n",
        "        # YOUR CODE HERE:\n",
        "        x = ...\n",
        "\n",
        "        return x"
      ],
      "id": "polished-clearing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "editorial-excitement"
      },
      "source": [
        "# Check:\n",
        "hidden_dim = 10\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "\n",
        "gnn = GNN(\n",
        "    input_dim=dataset.num_node_features, \n",
        "    hidden_dim=hidden_dim, \n",
        "    output_dim=dataset.num_classes, \n",
        "    num_heads=num_heads, \n",
        "    num_layers=num_layers\n",
        ")\n",
        "\n",
        "batch = loader.__iter__().next()\n",
        "output = gnn(batch)\n",
        "\n",
        "if output.shape == torch.Size([loader.batch_size, dataset.num_classes]): \n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Error: GNN should output a 2-dimensional tensor of shape (batch_size, num_classes)')"
      ],
      "id": "editorial-excitement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "personalized-chess"
      },
      "source": [
        "### Task 2.3 – Train GNN with GAT to predict classes of protein graphs"
      ],
      "id": "personalized-chess"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affected-norfolk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create GNN\n",
        "# YOUR CODE HERE:\n",
        "model = ...\n",
        "optimizer = ..."
      ],
      "id": "affected-norfolk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incorrect-ottawa"
      },
      "source": [
        "Training:"
      ],
      "id": "incorrect-ottawa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "continent-groove"
      },
      "source": [
        "epochs = 50\n",
        "model, train_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, epochs)\n",
        "plot_progress(train_loss, val_accuracy)"
      ],
      "id": "continent-groove",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comfortable-benchmark"
      },
      "source": [
        "Evaluation:"
      ],
      "id": "comfortable-benchmark"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greenhouse-dictionary"
      },
      "source": [
        "accuracy = evaluate(model, test_loader)\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "if accuracy >= 0.7:\n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Try better!')"
      ],
      "id": "greenhouse-dictionary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "graduate-snake"
      },
      "source": [
        "# Graph regression: QM9 dataset"
      ],
      "id": "graduate-snake"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVAY39SUsssW"
      },
      "source": [
        "QM9 is a [molecular dataset](https://www.nature.com/articles/sdata201422) standardized in machine learning as a chemical property prediction benchmark. It consists of small molecules (up to 29 atoms per molecule). Atoms contain positional coordinates embedded in a 3D space, a one-hot encoding vector that defines the type of molecule (H, C, N, O, F) and an integer value with the atom charge. For each molecule, authors of the dataset provide computed geometries minimal in energy, corresponding harmonic frequencies, dipole moments, polarizabilities, along with energies, enthalpies, and free energies of atomization. Any of these values can be considered as targets in the graph regression problem. In this seminar, we will predict one of them, the energy of the highest occupied molecular orbital $\\epsilon_{\\text{HOMO}}$.\n",
        "\n",
        "There are examples of some molecules from QM9 constructed using [PyMOL](https://pymol.org/2/):"
      ],
      "id": "gVAY39SUsssW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5W7VXeDp3As"
      },
      "source": [
        "<img src=\"https://i.ibb.co/qDv7Xy0/qm9-examples-v2.jpg\" alt=\"qm9-examples-v2\" border=\"0\">"
      ],
      "id": "A5W7VXeDp3As"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brief-nelson"
      },
      "source": [
        "from torch_geometric.datasets import QM9"
      ],
      "id": "brief-nelson",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "genetic-wealth"
      },
      "source": [
        "# In this seminar we will consider only a small part of this dataset\n",
        "# (in total it contains ~134k molecules)\n",
        "dataset = QM9(root='data/QM9')[:10000]"
      ],
      "id": "genetic-wealth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medieval-hopkins"
      },
      "source": [
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Number of node features: {dataset.num_features}')"
      ],
      "id": "medieval-hopkins",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respective-broad"
      },
      "source": [
        "n_examples = 5\n",
        "atom_types = ['H', 'C', 'N', 'O', 'F']\n",
        "\n",
        "fix, ax = plt.subplots(nrows=1, ncols=n_examples, figsize=(5*n_examples, 5))\n",
        "\n",
        "for i, rand_ix in enumerate(np.random.choice(dataset.indices(), 5)):\n",
        "    curr_ax = ax[i]\n",
        "    curr_ax.set_title(f'Graph with id={rand_ix}')\n",
        "    \n",
        "    pg_graph = dataset[rand_ix]\n",
        "    nx_graph = to_networkx(pg_graph, to_undirected=True)\n",
        "    colors = [COLORS[np.argmax(features[:4])] for features in pg_graph.x]\n",
        "    labels = {\n",
        "        i: atom_types[np.argmax(features[:4])]\n",
        "        for i, features in enumerate(pg_graph.x)\n",
        "    }\n",
        "    \n",
        "    draw_colored_graph(nx_graph, colors, labels=labels, ax=curr_ax)"
      ],
      "id": "respective-broad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "according-accent"
      },
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "data_size = len(dataset)\n",
        "\n",
        "train_loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset[int(data_size * 0.8):int(data_size * 0.9)], batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=batch_size)"
      ],
      "id": "according-accent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NfCqTkoXTQj"
      },
      "source": [
        "### Task 3.1 – Implement GNN for graph regresion"
      ],
      "id": "6NfCqTkoXTQj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR3TF5UhXkbL"
      },
      "source": [
        "Architecture is up to you. Feel free to use GAT or GCN as well as [any other](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers) layers."
      ],
      "id": "bR3TF5UhXkbL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "complicated-hindu"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads=1, dropout=0.0):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, data):\n",
        "        # YOUR CODE HERE"
      ],
      "id": "complicated-hindu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musical-cruise"
      },
      "source": [
        "# Check:\n",
        "hidden_dim = 10\n",
        "num_layers = 3\n",
        "\n",
        "gnn = GNN(\n",
        "    input_dim=dataset.num_node_features, \n",
        "    hidden_dim=hidden_dim, \n",
        "    output_dim=dataset.num_classes, \n",
        "    num_layers=num_layers\n",
        ")\n",
        "\n",
        "rand_idx = np.random.randint(0, len(train_loader))\n",
        "graph = train_loader.__iter__().next()\n",
        "\n",
        "output = gnn(graph)\n",
        "\n",
        "if output.shape == torch.Size([train_loader.batch_size, dataset.num_classes]): \n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Error: GNN should output a 2-dimensional tensor of shape (batch_size, num_classes)')"
      ],
      "id": "musical-cruise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97mvIIGiZBUC"
      },
      "source": [
        "### Task 3.2 – Train GNN\n",
        "\n",
        "Keep in mind that now we'are going to predict a real value instead of class so we need to change loss function and the way we evaluate the model."
      ],
      "id": "97mvIIGiZBUC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "negative-sapphire"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def mse_loss(predictions, targets):\n",
        "    # YOUR CODE HERE:\n",
        "    return ...\n",
        "\n",
        "\n",
        "# Wrapper for different architectures\n",
        "def model_forward(model, batch):\n",
        "    if model.__class__.__name__ == 'GNN':\n",
        "        return model(batch)\n",
        "    if model.__class__.__name__ == 'SchNet':\n",
        "        return model(batch.z, batch.pos, batch.batch)\n",
        "    raise Exception('Unknown model')\n",
        "\n",
        "\n",
        "def train_graph_regression(model, optimizer, train_loader, val_loader, target_ix, epochs):\n",
        "    train_mse = []\n",
        "    val_mse = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_mse = []\n",
        "        batch_val_mse = []\n",
        "\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch} train'):\n",
        "            # Get model predictions\n",
        "            # YOUR CODE HERE:\n",
        "            predictions = ...\n",
        "            \n",
        "            # Get ground-truth values\n",
        "            # YOUR CODE HERE:\n",
        "            targets = ...\n",
        "            \n",
        "            # Calculate loss\n",
        "            # YOUR CODE HERE:\n",
        "            loss = ...\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_train_mse.append(float(loss.data.numpy()))\n",
        "\n",
        "        train_mse.append(np.mean(batch_train_mse))\n",
        "\n",
        "        model.eval()\n",
        "        for batch in tqdm(val_loader, desc=f'Epoch {epoch} valid'):\n",
        "            predictions = model_forward(model, batch).squeeze()\n",
        "            targets = batch.y[:, target_ix].squeeze()\n",
        "            batch_val_mse.append(mse_loss(predictions, targets).detach().numpy())\n",
        "\n",
        "        val_mse.append(np.mean(batch_val_mse))\n",
        "        \n",
        "    return model, train_mse, val_mse\n",
        "\n",
        "\n",
        "def plot_progress(train_mse, val_mse):\n",
        "    fig, (train_ax, val_ax) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "\n",
        "    train_ax.plot(train_mse)\n",
        "    train_ax.set_title('Train MSE')\n",
        "    train_ax.set_xlabel('Epoch')\n",
        "\n",
        "    val_ax.plot(val_mse)\n",
        "    val_ax.set_title('Val MSE')\n",
        "    val_ax.set_xlabel('Epoch')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def evaluate_graph_regression(model, loader, target_ix):\n",
        "    loss = []\n",
        "    model.eval()\n",
        "    for batch in tqdm(loader):\n",
        "\n",
        "        # Get predicted values\n",
        "        # YOUR CODE HERE:\n",
        "        pred = ...\n",
        "\n",
        "        # Get ground-truth values\n",
        "        # YOUR CODE HERE:\n",
        "        true = ...\n",
        "\n",
        "        loss.append(mse_loss(pred, true).detach().numpy())\n",
        "\n",
        "    return np.mean(loss)"
      ],
      "id": "negative-sapphire",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indonesian-april"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create GNN\n",
        "# YOUR CODE HERE:\n",
        "model = ...\n",
        "optimizer = ..."
      ],
      "id": "indonesian-april",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "based-innocent"
      },
      "source": [
        "# In this seminar we will predict the HOMO energy.\n",
        "# This value goes the 3rd in the list of all targets in the PyG QM9 dataset\n",
        "# For details, see https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.QM9\n",
        "target_homo_ix = 2\n",
        "\n",
        "epochs = 10\n",
        "model, train_mse, val_mse = train_graph_regression(\n",
        "    model, \n",
        "    optimizer, \n",
        "    train_loader, \n",
        "    val_loader, \n",
        "    target_homo_ix, \n",
        "    epochs\n",
        ")\n",
        "plot_progress(train_mse, val_mse)"
      ],
      "id": "based-innocent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-yacht"
      },
      "source": [
        "loss = evaluate_graph_regression(model, test_loader, target_homo_ix)\n",
        "\n",
        "print('MSE:', loss)\n",
        "if loss <= 0.2:\n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Try better!')"
      ],
      "id": "sustained-yacht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ565WqMctfI"
      },
      "source": [
        "# SchNet and Equivariance\n",
        "\n",
        "* [original paper](https://arxiv.org/abs/1706.08566)\n",
        "* [original code](https://github.com/atomistic-machine-learning/SchNet)\n",
        "\n",
        "SchNet is one of the first GCN models where authors attempted to take into accout geometry of the underlying data. \n",
        "\n",
        "In case of molecular graphs, each node $v_i$ of graph $G$ is an atom that is characterized by its vector of features $\\boldsymbol{z}_i\\in\\mathbb{R}^d$ and by its position $\\boldsymbol{r}_i\\in\\mathbb{R}^3$. In general, the graph itself may not include the information about spatial relations between atoms. To take it into account, rotation-invariant _continuous-filter convolutions_ were proposed:\n",
        "\n",
        "$$\n",
        "z'_i=\\sum_{j=1}^Nz_j\\circ\\boldsymbol{W}(\\boldsymbol{r_i}-\\boldsymbol{r}_j),\\tag{9}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{W}(\\boldsymbol{r_i}-\\boldsymbol{r}_j)$ is a trainable and relative-distance-dependent filter, and $\\circ$ denotes element-wise multiplication.\n",
        "\n",
        "One of the most important aspects of learning on 3D objects is the fact that in most cases we do not have the fixed and preferred global orientation. When constructing geometric-learning models, one should take it into account and impose additional symmetry-related constraints on the model. This constraint is called $equivariance$ can be required with respect to rotations and translations in case of 3D space. More formally, having some group $G$ (e.g. group of rotations in 3D) and a function $f:X\\to Y$, this function is called $G$-$equivariant$ if for any $x\\in{X}$ and for any $g\\in{G}$\n",
        "\n",
        "$$\n",
        "f(\\rho_X(g)(x))=\\rho_Y(g)(f(x)),\\tag{10}\n",
        "$$\n",
        "\n",
        "where $\\rho_X:G\\to{GL}(X)$ and $\\rho_Y:G\\to{GL}(Y)$ are representations of group $G$ on spaces $X$ and $Y$ respectively. In case if $\\rho_Y=1$, function $f$ is called $G$-$invariant$.\n",
        "\n",
        "In case of (9), filters $\\boldsymbol{W}(\\boldsymbol{r_i}-\\boldsymbol{r}_j)$ are constructed to be rotation- and translation-invariant. To do it, we will make them depend only on distances between molecules:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{W}(\\boldsymbol{r_i}-\\boldsymbol{r}_j)=\\boldsymbol{W}(\\|\\boldsymbol{r_i}-\\boldsymbol{r}_j\\|).\\tag{11}\n",
        "$$\n",
        "\n",
        "In SchNet, these filters are constructed as MLPs that operate on $K$-dimensional radial basis finctions $\\{\\boldsymbol{e}_{ij}\\}$:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{W}(\\boldsymbol{r_i}-\\boldsymbol{r}_j)=\n",
        "\\sigma(\\boldsymbol{W}_2\\sigma(\\boldsymbol{W}_1\\boldsymbol{e}_{ij})),\\tag{12}\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "\\boldsymbol{e}_{ij}=\\big(e_{ij}^{(1)},\\dots,e_{ij}^{(K)}\\big)^{\\text{T}}, \\tag{13}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "e_{ij}^{(k)}=exp\\big(-\\gamma\\big[\\|\\boldsymbol{r}_i-\\boldsymbol{r}_j\\|-\\mu_k\\big]^2\\big).\\tag{14}\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ is non-linearity, $K$, $\\gamma$ and $\\{\\mu_k\\}$ are adjustable hyperparameters and $\\boldsymbol{W}_1$ and $\\boldsymbol{W}_2$ are trainable matrices.\n",
        "\n",
        "In the original paper, $\\sigma(x)=\\ln(0.5e^x+0.5)$, $K=300$, $0Å\\leq\\mu_k\\leq30Å$ every $0.1$Å, and $\\gamma=10$Å."
      ],
      "id": "qQ565WqMctfI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCoRI1xpohJj"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/7134790/65094886-d8dda800-d9f9-11e9-92e8-2737c4913ab7.png\" width=\"800\"/>\n",
        "</div>"
      ],
      "id": "WCoRI1xpohJj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRPhCGHpZRZ"
      },
      "source": [
        "Here we will not implement SchNet by ourselves, but we can [import it from PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.SchNet) and train on QM9."
      ],
      "id": "dGRPhCGHpZRZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drawn-graham"
      },
      "source": [
        "from torch_geometric.nn import SchNet\n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = ...\n",
        "optimizer = ..."
      ],
      "id": "drawn-graham",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "tutorial-tooth"
      },
      "source": [
        "epochs = 10\n",
        "target_homo_ix = 2\n",
        "model, train_mse, val_mse = train_graph_regression(\n",
        "    model, \n",
        "    optimizer, \n",
        "    train_loader, \n",
        "    val_loader, \n",
        "    target_homo_ix, \n",
        "    epochs\n",
        ")\n",
        "plot_progress(train_mse, val_mse)"
      ],
      "id": "tutorial-tooth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "posted-dispute"
      },
      "source": [
        "loss = evaluate(model, test_loader, target_homo_ix)\n",
        "\n",
        "print('MSE:', loss)\n",
        "if loss <= 0.2:\n",
        "    print('Good job!')\n",
        "else:\n",
        "    print('Try better!')"
      ],
      "id": "posted-dispute",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFeiS0Olrq_X"
      },
      "source": [
        "# Further reading\n",
        "\n",
        "In order to read more about geometric learning, message passing and equivariance, we refer to the following sources:\n",
        "* [Geometric Learning](https://arxiv.org/abs/2104.13478)\n",
        "* [Directional Message Passing](https://arxiv.org/abs/2003.03123)\n",
        "* [E(n)-equivariant graph neural networks](https://arxiv.org/abs/2102.09844)\n",
        "* [SE(3)-Transformers](https://arxiv.org/abs/2006.10503)"
      ],
      "id": "cFeiS0Olrq_X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funky-language"
      },
      "source": [
        ""
      ],
      "id": "funky-language",
      "execution_count": null,
      "outputs": []
    }
  ]
}